{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    @title: text-cnn 进行文本分类 \\n    @name: King\\n    @url: https://github.com/km1994/DataWhale_Study_Wp/tree/master/t7CNN/text-classification-cnn-rnn\\n    @datasets: THUCNews 文本分类数据集 \\n        （类别：体育, 财经, 房产, 家居, 教育, 科技, 时尚, 时政, 游戏, 娱乐）\\n        （链接: https://pan.baidu.com/s/1hugrfRu 密码: qfud）\\n    @decs: \\n            利用 text-CNN 进行文本分类\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    @title: text-cnn 进行文本分类 \n",
    "    @name: King\n",
    "    @url: https://github.com/km1994/DataWhale_Study_Wp/tree/master/t7CNN/text-classification-cnn-rnn\n",
    "    @datasets: THUCNews 文本分类数据集 \n",
    "        （类别：体育, 财经, 房产, 家居, 教育, 科技, 时尚, 时政, 游戏, 娱乐）\n",
    "        （链接: https://pan.baidu.com/s/1hugrfRu 密码: qfud）\n",
    "    @decs: \n",
    "            利用 text-CNN 进行文本分类\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、导入常用包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import tensorflow.contrib.keras as kr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、基础变量设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../resource/THUCNews_ch'\n",
    "train_dir = os.path.join(base_dir, 'cnews.train.txt')\n",
    "test_dir = os.path.join(base_dir, 'cnews.test.txt')\n",
    "val_dir = os.path.join(base_dir, 'cnews.val.txt')\n",
    "vocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')\n",
    "\n",
    "save_dir = 'checkpoints/textcnn'\n",
    "save_path = os.path.join(save_dir, 'best_validation')  # 最佳验证结果保存路径\n",
    "\n",
    "mode = 'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、TCNN 设置类定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNNConfig(object):\n",
    "    \"\"\"CNN配置参数\"\"\"\n",
    "\n",
    "    embedding_dim = 64  # 词向量维度\n",
    "    seq_length = 600  # 序列长度\n",
    "    num_classes = 10  # 类别数\n",
    "    num_filters = 256  # 卷积核数目\n",
    "    kernel_size = 5  # 卷积核尺寸\n",
    "    vocab_size = 5000  # 词汇表大小\n",
    "\n",
    "    hidden_dim = 128  # 全连接层神经元\n",
    "\n",
    "    dropout_keep_prob = 0.5  # dropout保留比例\n",
    "    learning_rate = 1e-3  # 学习率\n",
    "\n",
    "    batch_size = 64  # 每批训练大小\n",
    "    num_epochs = 10  # 总迭代轮次\n",
    "\n",
    "    print_per_batch = 100  # 每多少轮输出一次结果\n",
    "    save_per_batch = 10  # 每多少轮存入tensorboard\n",
    "\n",
    "config = TCNNConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、cnews 数据集处理常用方法定义\n",
    "\n",
    "## 4.1 基础工具方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.version_info[0] > 2:\n",
    "    is_py3 = True\n",
    "else:\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding(\"utf-8\")\n",
    "    is_py3 = False\n",
    "\n",
    "# 如果在python2下面使用python3训练的模型，可考虑调用此函数转化一下字符编码\n",
    "def native_word(word, encoding='utf-8'):\n",
    "    \"\"\"如果在python2下面使用python3训练的模型，可考虑调用此函数转化一下字符编码\"\"\"\n",
    "    if not is_py3:\n",
    "        return word.encode(encoding)\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def native_content(content):\n",
    "    if not is_py3:\n",
    "        return content.decode('utf-8')\n",
    "    else:\n",
    "        return content\n",
    "\n",
    "# 常用文件操作，可在python2和python3间切换.\n",
    "def open_file(filename, mode='r'):\n",
    "    \"\"\"\n",
    "    常用文件操作，可在python2和python3间切换.\n",
    "    mode: 'r' or 'w' for read or write\n",
    "    \"\"\"\n",
    "    if is_py3:\n",
    "        return open(filename, mode, encoding='utf-8', errors='ignore')\n",
    "    else:\n",
    "        return open(filename, mode)\n",
    "\n",
    "# 读取文件数据\n",
    "def read_file(filename):\n",
    "    \"\"\"读取文件数据\"\"\"\n",
    "    contents, labels = [], []\n",
    "    with open_file(filename) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                label, content = line.strip().split('\\t')\n",
    "                if content:\n",
    "                    contents.append(list(native_content(content)))\n",
    "                    labels.append(native_content(label))\n",
    "            except:\n",
    "                pass\n",
    "    return contents, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 文本分词方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据分词，及停用词清除\n",
    "import jieba\n",
    "def jieba_cut_word(data_list,stopword_path,cut_word_to_str_flag = True):\n",
    "    '''\n",
    "    数据分词，及停用词清除\n",
    "    :param data_list:               list  需要处理的数据列表\n",
    "    :param stopword_path:           String 停用词文件\n",
    "    :param cut_word_to_str_flag:    True 切分后的句子转化为字符串， False 切分后的句子转化为列表\n",
    "    :return:\n",
    "        docs_list   List    处理后的数据列表\n",
    "    '''\n",
    "    # 1.读取停用词文件\n",
    "    with open_file(stopword_path) as f_stop:\n",
    "        try:\n",
    "            f_stop_text = f_stop.read()\n",
    "        finally:\n",
    "            f_stop.close()\n",
    "    # 停用词清除\n",
    "    f_stop_seg_list = f_stop_text.split('\\n')\n",
    "\n",
    "    # 2.文本分词处理，并进行清除停用词处理\n",
    "    docs_list = []\n",
    "    #print(\"data_list:{0}\".format(data_list[0:2]))\n",
    "    for line in data_list:\n",
    "        seg_list = jieba.cut(line, cut_all=False)\n",
    "        word_list = list(seg_list)\n",
    "        mywordlist = []\n",
    "        for myword in word_list:\n",
    "            if not (myword in f_stop_seg_list):\n",
    "                mywordlist.append(myword)\n",
    "\n",
    "        if cut_word_to_str_flag:\n",
    "            mywordlist =\" \".join(mywordlist)\n",
    "\n",
    "        docs_list.append(mywordlist)\n",
    "\n",
    "    return docs_list\n",
    "\n",
    "# 读取文件数据,并利用结巴分词进行分词\n",
    "def read_file_to_words(filename):\n",
    "    \"\"\"读取文件数据\"\"\"\n",
    "    contents, labels = [], []\n",
    "    with open_file(filename) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                label, content = line.strip().split('\\t')\n",
    "                if content:\n",
    "                    contents.append(''.join(list(native_content(content))))\n",
    "                    labels.append(native_content(label))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # 分词处理\n",
    "    stopword_path = \"../../resource/stopwords.txt\"\n",
    "    train_docs_list = jieba_cut_word(contents, stopword_path, cut_word_to_str_flag=False)\n",
    "    return train_docs_list, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 词汇表构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据训练集构建词汇表，存储\n",
    "def build_vocab_to_words(train_dir, vocab_dir, vocab_size=5000):\n",
    "    \"\"\"根据训练集构建词汇表，存储\"\"\"\n",
    "    print(\"------------build_vocab_to_words begin-------------\")\n",
    "    data_train, _ = read_file_to_words(train_dir)\n",
    "\n",
    "    all_data = []\n",
    "    for content in data_train:\n",
    "        all_data.extend(content)\n",
    "\n",
    "    counter = Counter(all_data)\n",
    "    count_pairs = counter.most_common(vocab_size - 1)\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    # 添加一个 <PAD> 来将所有文本pad为同一长度\n",
    "    words = ['<PAD>'] + list(words)\n",
    "    open_file(vocab_dir, mode='w').write('\\n'.join(words) + '\\n')\n",
    "    print(\"------------build_vocab_to_words end-------------\")\n",
    "\n",
    "#根据训练集构建词汇表，存储\n",
    "def build_vocab(train_dir, vocab_dir, vocab_size=5000):\n",
    "    \"\"\"根据训练集构建词汇表，存储\"\"\"\n",
    "    data_train, _ = read_file(train_dir)\n",
    "\n",
    "    all_data = []\n",
    "    for content in data_train:\n",
    "        all_data.extend(content)\n",
    "\n",
    "    counter = Counter(all_data)\n",
    "    count_pairs = counter.most_common(vocab_size - 1)\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    # 添加一个 <PAD> 来将所有文本pad为同一长度\n",
    "    words = ['<PAD>'] + list(words)\n",
    "    open_file(vocab_dir, mode='w').write('\\n'.join(words) + '\\n')\n",
    "    '''\n",
    "        print(\"data_train[0:1]:{0}\".format(data_train[0:1]))\n",
    "        print(\"all_data[0:1]:{0}\".format(all_data[0:1]))\n",
    "        print(\"counter:{0}\".format(counter))\n",
    "        print(\"count_pairs:{0}\".format(count_pairs))\n",
    "        print(\"words[0:1]:{0}\".format(words[0:1]))\n",
    "        output:\n",
    "            data_train[0:1]:[['马', '晓', '旭', '意',..., '有', '些', '不', '解', '。']]\n",
    "            all_data[0:1]:['马']\n",
    "            counter:Counter({'，': 1871208, '的': 1414310, ... , '溟': 1})\n",
    "            count_pairs:[('，', 1871208), ('的', 1414310), ('。', 822140),..., ('箕', 9), ('柘', 9)]\n",
    "            words[0:1]:['<PAD>']\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 词汇表操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取词汇表\n",
    "def read_vocab(vocab_dir):\n",
    "    \"\"\"读取词汇表\"\"\"\n",
    "    # words = open_file(vocab_dir).read().strip().split('\\n')\n",
    "    with open_file(vocab_dir) as fp:\n",
    "        # 如果是py2 则每个值都转化为unicode\n",
    "        words = [native_content(_.strip()) for _ in fp.readlines()]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id\n",
    "\n",
    "# 读取分类目录，固定\n",
    "def read_category():\n",
    "    \"\"\"读取分类目录，固定\"\"\"\n",
    "    categories = ['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
    "    categories = [native_content(x) for x in categories]\n",
    "    cat_to_id = dict(zip(categories, range(len(categories))))\n",
    "    '''\n",
    "        print(\"categories:{0}\".format(categories))\n",
    "        print(\"cat_to_id:{0}\".format(cat_to_id))\n",
    "        output: \n",
    "            categories:['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
    "            cat_to_id:{'体育': 0, '财经': 1, '房产': 2, '家居': 3, '教育': 4, '科技': 5, '时尚': 6, '时政': 7, '游戏': 8, '娱乐': 9}\n",
    "    '''\n",
    "    return categories, cat_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 id 和 文字间的相互转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将id表示的内容转换为文字\n",
    "def to_words(content, words):\n",
    "    \"\"\"将id表示的内容转换为文字\"\"\"\n",
    "    return ''.join(words[x] for x in content)\n",
    "\n",
    "# 将文件转换为id表示\n",
    "def process_file_to_words(filename, word_to_id, cat_to_id, max_length=600):\n",
    "    \"\"\"将文件转换为id表示\"\"\"\n",
    "    contents, labels = read_file_to_words(filename)\n",
    "\n",
    "    data_id, label_id = [], []\n",
    "    for i in range(len(contents)):\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n",
    "        label_id.append(cat_to_id[labels[i]])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, max_length)\n",
    "    y_pad = kr.utils.to_categorical(label_id, num_classes=len(cat_to_id))  # 将标签转换为one-hot表示\n",
    "\n",
    "    return x_pad, y_pad\n",
    "\n",
    "# 将文件转换为id表示\n",
    "def process_file(filename, word_to_id, cat_to_id, max_length=600):\n",
    "    \"\"\"将文件转换为id表示\"\"\"\n",
    "    contents, labels = read_file(filename)\n",
    "\n",
    "    data_id, label_id = [], []\n",
    "    for i in range(len(contents)):\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n",
    "        label_id.append(cat_to_id[labels[i]])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, max_length)\n",
    "    y_pad = kr.utils.to_categorical(label_id, num_classes=len(cat_to_id))  # 将标签转换为one-hot表示\n",
    "\n",
    "    return x_pad, y_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 生成 批次数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成批次数据\n",
    "def batch_iter(x, y, batch_size=64):\n",
    "    \"\"\"生成批次数据\"\"\"\n",
    "    data_len = len(x)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_shuffle = x[indices]\n",
    "    y_shuffle = y[indices]\n",
    "\n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、定义 Text-CNN 模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本分类，CNN模型\n",
    "class TextCNN(object):\n",
    "    \"\"\"文本分类，CNN模型\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        print(\"------------------Class TextCNN--------------------\")\n",
    "        self.config = config\n",
    "\n",
    "        # 三个待输入的数据\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, self.config.num_classes], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        self.cnn()\n",
    "\n",
    "    def cnn(self):\n",
    "        \"\"\"CNN模型\"\"\"\n",
    "        # 词向量映射\n",
    "        with tf.device('/cpu:0'):\n",
    "            embedding = tf.get_variable('embedding', [self.config.vocab_size, self.config.embedding_dim])\n",
    "            embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)\n",
    "\n",
    "        with tf.name_scope(\"cnn\"):\n",
    "            # CNN layer\n",
    "            conv = tf.layers.conv1d(embedding_inputs, self.config.num_filters, self.config.kernel_size, name='conv')\n",
    "            # global max pooling layer\n",
    "            gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')\n",
    "\n",
    "        with tf.name_scope(\"score\"):\n",
    "            # 全连接层，后面接dropout以及relu激活\n",
    "            fc = tf.layers.dense(gmp, self.config.hidden_dim, name='fc1')\n",
    "            fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n",
    "            fc = tf.nn.relu(fc)\n",
    "\n",
    "            # 分类器\n",
    "            self.logits = tf.layers.dense(fc, self.config.num_classes, name='fc2')\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)  # 预测类别\n",
    "\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            # 损失函数，交叉熵\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            # 优化器\n",
    "            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # 准确率\n",
    "            correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、训练 or 测试方法定义\n",
    "\n",
    "## 6.1 工具方法模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取已使用时间\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "def feed_data(x_batch, y_batch, keep_prob):\n",
    "    feed_dict = {\n",
    "        model.input_x: x_batch,\n",
    "        model.input_y: y_batch,\n",
    "        model.keep_prob: keep_prob\n",
    "    }\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 评估模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估在某一数据上的准确率和损失\n",
    "def evaluate(sess, x_, y_):\n",
    "    \"\"\"评估在某一数据上的准确率和损失\"\"\"\n",
    "    data_len = len(x_)\n",
    "    batch_eval = batch_iter(x_, y_, 128)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        feed_dict = feed_data(x_batch, y_batch, 1.0)\n",
    "        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "        total_loss += loss * batch_len\n",
    "        total_acc += acc * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 模型训练模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train():\n",
    "    print(\"Configuring TensorBoard and Saver...\")\n",
    "    # 配置 Tensorboard，重新训练时，请将tensorboard文件夹删除，不然图会覆盖\n",
    "    tensorboard_dir = 'tensorboard/textcnn'\n",
    "    if not os.path.exists(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "\n",
    "    tf.summary.scalar(\"loss\", model.loss)\n",
    "    tf.summary.scalar(\"accuracy\", model.acc)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "\n",
    "    # 配置 Saver\n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    print(\"Loading training and validation data...\")\n",
    "    # 载入训练集与验证集\n",
    "    start_time = time.time()\n",
    "    x_train, y_train = process_file(train_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    print(\"x_train[0:1]：{0}\".format(x_train[0:1]))\n",
    "    print(\"len(x_train[0]):{0}\".format(len(x_train[0])))\n",
    "    x_val, y_val = process_file(val_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "    # 创建session\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    print('Training and evaluating...')\n",
    "    start_time = time.time()\n",
    "    total_batch = 0  # 总批次\n",
    "    best_acc_val = 0.0  # 最佳验证集准确率\n",
    "    last_improved = 0  # 记录上一次提升批次\n",
    "    require_improvement = 1000  # 如果超过1000轮未提升，提前结束训练\n",
    "\n",
    "    flag = False\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        batch_train = batch_iter(x_train, y_train, config.batch_size)\n",
    "        for x_batch, y_batch in batch_train:\n",
    "            feed_dict = feed_data(x_batch, y_batch, config.dropout_keep_prob)\n",
    "\n",
    "            if total_batch % config.save_per_batch == 0:\n",
    "                # 每多少轮次将训练结果写入tensorboard scalar\n",
    "                s = session.run(merged_summary, feed_dict=feed_dict)\n",
    "                writer.add_summary(s, total_batch)\n",
    "\n",
    "            if total_batch % config.print_per_batch == 0:\n",
    "                # 每多少轮次输出在训练集和验证集上的性能\n",
    "                feed_dict[model.keep_prob] = 1.0\n",
    "                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "                loss_val, acc_val = evaluate(session, x_val, y_val)  # todo\n",
    "\n",
    "                if acc_val > best_acc_val:\n",
    "                    # 保存最好结果\n",
    "                    best_acc_val = acc_val\n",
    "                    last_improved = total_batch\n",
    "                    saver.save(sess=session, save_path=save_path)\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                      + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n",
    "\n",
    "            session.run(model.optim, feed_dict=feed_dict)  # 运行优化\n",
    "            total_batch += 1\n",
    "\n",
    "            if total_batch - last_improved > require_improvement:\n",
    "                # 验证集正确率长期不提升，提前结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break  # 跳出循环\n",
    "        if flag:  # 同上\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型测试\n",
    "def test():\n",
    "    print(\"Loading test data...\")\n",
    "    start_time = time.time()\n",
    "    x_test, y_test = process_file(test_dir, word_to_id, cat_to_id, config.seq_length)\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess=session, save_path=save_path)  # 读取保存的模型\n",
    "\n",
    "    print('Testing...')\n",
    "    loss_test, acc_test = evaluate(session, x_test, y_test)\n",
    "    msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n",
    "    print(msg.format(loss_test, acc_test))\n",
    "\n",
    "    batch_size = 128\n",
    "    data_len = len(x_test)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    y_test_cls = np.argmax(y_test, 1)\n",
    "    y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # 保存预测结果\n",
    "    for i in range(num_batch):  # 逐批次处理\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        feed_dict = {\n",
    "            model.input_x: x_test[start_id:end_id],\n",
    "            model.keep_prob: 1.0\n",
    "        }\n",
    "        y_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "    # 评估\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories))\n",
    "\n",
    "    # 混淆矩阵\n",
    "    print(\"Confusion Matrix...\")\n",
    "    cm = metrics.confusion_matrix(y_test_cls, y_pred_cls)\n",
    "    print(cm)\n",
    "\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七、程序入口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring CNN model...\n",
      "------------------Class TextCNN--------------------\n",
      "WARNING:tensorflow:From <ipython-input-16-d14b481865d2>:41: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Configuring TensorBoard and Saver...\n",
      "Loading training and validation data...\n",
      "x_train[0:1]：[[1609  659   56    8   14 1190    1  108 1135  121  244 1564   20  951\n",
      "     2  977  851  194  165    8  264   32  330  464  900 1478    3   61\n",
      "   951   91  164    1  143  157  244 1296  271  977  851   57   27    1\n",
      "    14 1190  167   63   61   10  385   22  122   27    1   80  505 1055\n",
      "  1342  165    8  886   61   34    2  215  730    3 1550  205  538    4\n",
      "   538    2  608  144    1  157  244   72  404   10  143  125   61  951\n",
      "     2  644   36  977  851    1   18   55   52  883   66  202   10    1\n",
      "   125  405  165    8  330  464  490  121    2 1278  554    1   21   10\n",
      "   232  797  157  200   40    1   16  725  244  526  126   11  853  143\n",
      "   125    2  977  851    1  117  244  371  534 1404  267 1069  832    3\n",
      "     6 1190   11  977  851   39  589  157  244   34   84  194    9    5\n",
      "   421  217 1712 1993  182    1  108    6  725  492   35  534   86   72\n",
      "   404  100   65    1  117  244  326   68   23 1950 1052   24   10    3\n",
      "     6 1609  659   71   59    4  309    2  977  851    1   16  725  244\n",
      "   321  332   41  232  297   54    8    2  157  200    9  333   33   54\n",
      "   215  110    2  814  931  162  477   31  831  120  593  247  253   81\n",
      "   212    1  166  158   19    4 1015  576  718  239  977  851  264  814\n",
      "    15  718  239  242 1569  151 1762  931    2   33   54  230  244 1564\n",
      "   358    6   10  161  143  155   41    2  172  555    3   80 1296  271\n",
      "  1609  659  100   59    1   11   59  699 1361 2409 1584   56    4  339\n",
      "   165    8  977  851    1 1361 2409 1584    5  105   70   18  105   62\n",
      "     6  132  744 1533   20   10  242 1569    1  166  158   46  165    8\n",
      "   493  115   18   77   62  654 1269  257  703  470    2  422  252  212\n",
      "     3  244 1564  639  845   84    1 1361 2409 1584  194  165    8   33\n",
      "    54   37 1807 1758  721    1  108   21   10  324  117  220  503    1\n",
      "    19  173  125   96    5  219   44 1084 1012  961  365    1  151  242\n",
      "  1569 1464  611  121   10  100   59  333 1434  562  977  851    3  269\n",
      "    60    8   10 1361 2409 1584   19   22  644  139    1  166  158   16\n",
      "   725  244   39 1190   11  977  851   56  336   68  170  316 1619 1524\n",
      "     1  109   41    5  747  169  157  200   40  264 1931   80  545   37\n",
      "   242 1569    1  345   50  282  283 1039  769  200    3   80  291  589\n",
      "   244  200  387 1197 2173    6  422  252  212   11  264  814  293  610\n",
      "   245 1462  725  492    2   65  228    1   46  219    6 1609  659    2\n",
      "    16  725  244   54    6  231  110  981 2629    1   23  977  851   11\n",
      "     9  631 3221 4007  244  200   40   41  483  215  111   69    1   28\n",
      "    40   51    9   45  333   33   19  184    2  182  162   10    3   24\n",
      "     4  172  152   69   13  200  127  185    3  175  132  744   32 1609\n",
      "   659    1 1190  298    4  533 1333  546  205   16  725  244    1   23\n",
      "    46 2132   10    1   28   40  531   32  986  662 1190   56   61   32\n",
      "   986  662    1    6  132  744  385  130  977  851   88  230   14 1190\n",
      "   305 3009   30   10    1  165  265   32   34 1609  659  433 1635   32\n",
      "    19  187  182  162    3   24    4  172   16  725  157  200   46   39\n",
      "  1190  298    2   23  808 2025   24    8  156    9  311    3]]\n",
      "len(x_train[0]):600\n",
      "Time usage: 0:00:19\n",
      "Training and evaluating...\n",
      "Epoch: 1\n",
      "Iter:      0, Train Loss:    2.3, Train Acc:  15.62%, Val Loss:    2.3, Val Acc:  10.00%, Time: 0:00:06 *\n",
      "Iter:    100, Train Loss:   0.86, Train Acc:  78.12%, Val Loss:    1.1, Val Acc:  70.38%, Time: 0:00:49 *\n",
      "Iter:    200, Train Loss:   0.24, Train Acc:  92.19%, Val Loss:   0.62, Val Acc:  82.34%, Time: 0:01:32 *\n",
      "Iter:    300, Train Loss:    0.1, Train Acc:  95.31%, Val Loss:    0.4, Val Acc:  88.66%, Time: 0:02:14 *\n",
      "Iter:    400, Train Loss:    0.2, Train Acc:  93.75%, Val Loss:    0.3, Val Acc:  92.22%, Time: 0:02:57 *\n",
      "Iter:    500, Train Loss:    0.3, Train Acc:  92.19%, Val Loss:    0.3, Val Acc:  92.36%, Time: 0:03:40 *\n",
      "Iter:    600, Train Loss:   0.16, Train Acc:  95.31%, Val Loss:   0.27, Val Acc:  92.84%, Time: 0:04:22 *\n",
      "Iter:    700, Train Loss:   0.29, Train Acc:  92.19%, Val Loss:    0.3, Val Acc:  91.00%, Time: 0:05:05 \n",
      "Epoch: 2\n",
      "Iter:    800, Train Loss:   0.12, Train Acc:  95.31%, Val Loss:   0.21, Val Acc:  93.88%, Time: 0:05:48 *\n",
      "Iter:    900, Train Loss:  0.095, Train Acc:  98.44%, Val Loss:   0.25, Val Acc:  92.76%, Time: 0:06:30 \n",
      "Iter:   1000, Train Loss:   0.12, Train Acc:  96.88%, Val Loss:   0.28, Val Acc:  90.98%, Time: 0:07:13 \n",
      "Iter:   1100, Train Loss:   0.17, Train Acc:  96.88%, Val Loss:   0.26, Val Acc:  92.20%, Time: 0:07:55 \n",
      "Iter:   1200, Train Loss:   0.17, Train Acc:  93.75%, Val Loss:   0.25, Val Acc:  92.38%, Time: 0:08:38 \n",
      "Iter:   1300, Train Loss:  0.049, Train Acc:  98.44%, Val Loss:   0.23, Val Acc:  93.18%, Time: 0:09:20 \n",
      "Iter:   1400, Train Loss:   0.32, Train Acc:  93.75%, Val Loss:   0.21, Val Acc:  93.72%, Time: 0:10:03 \n",
      "Iter:   1500, Train Loss:    0.2, Train Acc:  90.62%, Val Loss:   0.25, Val Acc:  93.06%, Time: 0:10:46 \n",
      "Epoch: 3\n",
      "Iter:   1600, Train Loss:  0.014, Train Acc: 100.00%, Val Loss:   0.23, Val Acc:  93.16%, Time: 0:11:28 \n",
      "Iter:   1700, Train Loss:  0.052, Train Acc:  98.44%, Val Loss:   0.22, Val Acc:  92.60%, Time: 0:12:11 \n",
      "Iter:   1800, Train Loss:   0.11, Train Acc:  95.31%, Val Loss:   0.29, Val Acc:  91.60%, Time: 0:12:53 \n",
      "No optimization for a long time, auto-stopping...\n"
     ]
    }
   ],
   "source": [
    "print('Configuring CNN model...')\n",
    "config = TCNNConfig()                        # 加载配置文件\n",
    "if not os.path.exists(vocab_dir):           # 如果不存在词汇表，重建\n",
    "    build_vocab(train_dir, vocab_dir, config.vocab_size)\n",
    "categories, cat_to_id = read_category()     # 获取分类类别\n",
    "words, word_to_id = read_vocab(vocab_dir)   # 读取词汇表\n",
    "config.vocab_size = len(words)              # 修改 词汇表大小\n",
    "model = TextCNN(config)\n",
    "\n",
    "if mode == 'train':\n",
    "    train()\n",
    "else:\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
